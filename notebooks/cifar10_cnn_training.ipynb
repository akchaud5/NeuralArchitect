{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Project: CIFAR-10 Classification with CNN\n",
    "\n",
    "This notebook demonstrates how to use the neural networks framework to train a CNN model on the CIFAR-10 dataset for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.models.cnn_model import CNNModel\n",
    "from src.utils.trainer import Trainer\n",
    "from src.utils.metrics import MetricsTracker\n",
    "from src.config.config_manager import ConfigManager, get_default_config\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare CIFAR-10 Data\n",
    "\n",
    "CIFAR-10 is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations with data augmentation for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # CIFAR-10 mean and std\n",
    "])\n",
    "\n",
    "# Simpler transformation for testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10('../data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10('../data', train=False, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Some Examples from CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show images\n",
    "def imshow(img):\n",
    "    # Unnormalize\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2470, 0.2435, 0.2616])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# Get a batch of training data\n",
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "# Plot some examples\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(imshow(example_data[i]))\n",
    "    plt.title(f\"{train_dataset.classes[example_targets[i]]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Create the Model\n",
    "\n",
    "Let's use our configuration manager to set up the model parameters for CIFAR-10, which has color images (3 channels) and 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the default configuration\n",
    "config_manager = ConfigManager(default_config=get_default_config())\n",
    "config = config_manager.get_all()\n",
    "\n",
    "# Update configuration for CIFAR-10\n",
    "config_manager.set('model.input_channels', 3)  # CIFAR-10 images are RGB\n",
    "config_manager.set('model.num_classes', 10)  # 10 classes in CIFAR-10\n",
    "config_manager.set('cnn.conv_channels', [64, 128, 256])  # Deeper CNN architecture\n",
    "config_manager.set('cnn.fc_units', [512, 256])  # Larger fully-connected layers\n",
    "config_manager.set('model.dropout_rate', 0.5)  # Dropout for regularization\n",
    "config_manager.set('training.num_epochs', 20)  # Number of epochs\n",
    "config_manager.set('training.learning_rate', 0.001)  # Learning rate\n",
    "config_manager.set('training.weight_decay', 1e-4)  # Weight decay for regularization\n",
    "\n",
    "# Create model configuration\n",
    "model_config = {\n",
    "    'input_channels': config['model']['input_channels'],\n",
    "    'num_classes': config['model']['num_classes'],\n",
    "    'conv_channels': config['cnn']['conv_channels'],\n",
    "    'fc_units': config['cnn']['fc_units'],\n",
    "    'dropout_rate': config['model']['dropout_rate']\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = CNNModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"CNN Model created with {model.get_parameter_count():,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Trainer\n",
    "\n",
    "Now let's set up the training configuration and create our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer configuration\n",
    "trainer_config = {\n",
    "    'learning_rate': config['training']['learning_rate'],\n",
    "    'weight_decay': config['training']['weight_decay'],\n",
    "    'num_epochs': config['training']['num_epochs'],\n",
    "    'batch_size': batch_size,\n",
    "    'optimizer': 'adam',  # Use Adam optimizer\n",
    "    'scheduler': 'cosine',  # Use cosine annealing scheduler\n",
    "    'criterion': 'cross_entropy',  # Use cross-entropy loss\n",
    "    'clip_grad_norm': 1.0,  # Clip gradients\n",
    "    'early_stopping_patience': 5,  # Stop training if no improvement after 5 epochs\n",
    "    'checkpoint_dir': '../checkpoints/cifar10',  # Directory to save model checkpoints\n",
    "    'save_best_only': True  # Only save the best model\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(trainer_config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(model, trainer_config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we're ready to train our model on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(f\"Starting training for {trainer_config['num_epochs']} epochs...\")\n",
    "stats = trainer.train(train_loader, test_loader)\n",
    "\n",
    "# Print best results\n",
    "print(f\"\\nBest validation accuracy: {stats['best_val_acc']:.2f}%\")\n",
    "print(f\"Best validation loss: {stats['best_val_loss']:.4f} (epoch {stats['best_epoch']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Let's visualize how the training and validation metrics changed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss/accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(stats['train_loss']) + 1), stats['train_loss'], label='Training Loss')\n",
    "plt.plot(range(1, len(stats['val_loss']) + 1), stats['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(stats['train_acc']) + 1), stats['train_acc'], label='Training Accuracy')\n",
    "plt.plot(range(1, len(stats['val_acc']) + 1), stats['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot learning rate changes during training\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, len(stats['learning_rates']) + 1), stats['learning_rates'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Best Model and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = os.path.join(trainer_config['checkpoint_dir'], 'best_model.pt')\n",
    "model.load(best_model_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating the best model on the test set...\")\n",
    "test_loss, test_acc = trainer.evaluate(test_loader, desc=\"Test\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions and Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metrics tracker\n",
    "metrics_tracker = MetricsTracker(task_type='classification', n_classes=10)\n",
    "\n",
    "# Get a batch of test data\n",
    "test_examples = iter(test_loader)\n",
    "test_images, test_labels = next(test_examples)\n",
    "\n",
    "# Move to device\n",
    "test_images = test_images.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_images)\n",
    "    probabilities = torch.softmax(outputs, dim=1)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Update metrics tracker\n",
    "    metrics_tracker.update(test_labels, predicted, probabilities)\n",
    "\n",
    "# Evaluate the model on the entire test set and track metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update the metrics tracker\n",
    "        metrics_tracker.update(labels, predicted, probabilities)\n",
    "\n",
    "# Print detailed metrics\n",
    "metrics_tracker.print_metrics()\n",
    "\n",
    "# Move tensors back to CPU for plotting\n",
    "test_images = test_images.cpu()\n",
    "test_labels = test_labels.cpu()\n",
    "predicted = predicted.cpu()\n",
    "\n",
    "# Plot images with predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.imshow(imshow(test_images[i]))\n",
    "    title_color = 'green' if predicted[i] == test_labels[i] else 'red'\n",
    "    plt.title(f\"True: {train_dataset.classes[test_labels[i]]}\\nPred: {train_dataset.classes[predicted[i]]}\", \n",
    "              color=title_color)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize confusion matrix\n",
    "metrics = metrics_tracker.compute()\n",
    "confusion_mat = metrics['confusion_matrix']\n",
    "\n",
    "# Compute normalized confusion matrix\n",
    "confusion_mat_normalized = confusion_mat.astype('float') / confusion_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(confusion_mat_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(train_dataset.classes))\n",
    "plt.xticks(tick_marks, train_dataset.classes, rotation=45)\n",
    "plt.yticks(tick_marks, train_dataset.classes)\n",
    "\n",
    "# Add text annotations\n",
    "thresh = confusion_mat_normalized.max() / 2.\n",
    "for i in range(confusion_mat_normalized.shape[0]):\n",
    "    for j in range(confusion_mat_normalized.shape[1]):\n",
    "        plt.text(j, i, format(confusion_mat_normalized[i, j], '.2f'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if confusion_mat_normalized[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the Most Difficult Classes\n",
    "\n",
    "Let's analyze which classes are the most difficult for our model to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "per_class_accuracy = confusion_mat.diagonal() / confusion_mat.sum(axis=1)\n",
    "\n",
    "# Create a bar chart showing accuracy for each class\n",
    "plt.figure(figsize=(12, 6))\n",
    "classes = train_dataset.classes\n",
    "plt.bar(range(len(classes)), per_class_accuracy * 100)\n",
    "plt.xticks(range(len(classes)), classes, rotation=45)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add accuracy values on top of the bars\n",
    "for i, v in enumerate(per_class_accuracy):\n",
    "    plt.text(i, v * 100 + 1, f\"{v*100:.1f}%\", ha='center')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the most confused class pairs\n",
    "n_classes = len(classes)\n",
    "confusion_pairs = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        if i != j:\n",
    "            # True label i, predicted as j\n",
    "            confusion_pairs.append(((i, j), confusion_mat[i, j]))\n",
    "\n",
    "# Sort by number of confusions (descending)\n",
    "confusion_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top confused pairs\n",
    "print(\"Top confused class pairs (true -> predicted):\")\n",
    "for (true_label, pred_label), count in confusion_pairs[:10]:\n",
    "    print(f\"  {classes[true_label]} -> {classes[pred_label]}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model Configuration\n",
    "\n",
    "Let's save the model configuration for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration to a file\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "config_path = '../outputs/cifar10_cnn_config.yaml'\n",
    "config_manager.save_config(config_path)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how to use the neural networks framework to:\n",
    "\n",
    "1. Load and preprocess the CIFAR-10 dataset\n",
    "2. Configure and create a CNN model for image classification\n",
    "3. Train the model with appropriate hyperparameters\n",
    "4. Evaluate performance with various metrics\n",
    "5. Visualize predictions and identify difficult classes\n",
    "\n",
    "CIFAR-10 is more challenging than MNIST and even this relatively simple CNN architecture can achieve reasonable results. For better performance, more advanced architectures like ResNet or EfficientNet could be implemented by extending the BaseModel class in our framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}