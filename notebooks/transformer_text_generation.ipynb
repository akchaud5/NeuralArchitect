{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Project: Text Generation with Transformer\n",
    "\n",
    "This notebook demonstrates how to use the neural networks framework to train a Transformer model for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.models.transformer_model import TransformerModel\n",
    "from src.utils.trainer import Trainer\n",
    "from src.data.datasets import TextDataset\n",
    "from src.config.config_manager import ConfigManager, get_default_config\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Mock Tokenizer\n",
    "\n",
    "For demonstration purposes, we'll create a simple tokenizer. In a real application, you would use a more sophisticated tokenizer like those from the Hugging Face `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __call__(self, text, max_length=128, padding='max_length', truncation=True, return_tensors=None):\n",
    "        # This is a very simplified tokenization for demonstration purposes\n",
    "        # In a real scenario, you would use a proper tokenizer (e.g., from Hugging Face)\n",
    "        tokens = [hash(word) % (self.vocab_size - 4) + 4 for word in text.split()]\n",
    "        \n",
    "        # Add special tokens: 0=PAD, 1=BOS, 2=EOS, 3=UNK\n",
    "        tokens = [1] + tokens + [2]  # Add BOS and EOS tokens\n",
    "        \n",
    "        # Truncate if necessary\n",
    "        if truncation and len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length-1] + [2]  # Keep EOS token\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if padding == 'max_length':\n",
    "            tokens = tokens + [0] * (max_length - len(tokens))\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1 if token != 0 else 0 for token in tokens]\n",
    "        \n",
    "        # Convert to tensors if requested\n",
    "        if return_tensors == 'pt':\n",
    "            return {\n",
    "                'input_ids': torch.tensor([tokens]),\n",
    "                'attention_mask': torch.tensor([attention_mask])\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokens,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Mock Dataset\n",
    "\n",
    "Let's create a mock dataset for demonstration purposes. In a real application, you would load actual text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_dataset(num_samples, seq_length, vocab_size):\n",
    "    \"\"\"Generate a mock text dataset for demonstration purposes.\"\"\"\n",
    "    sequences = []\n",
    "    for _ in range(num_samples):\n",
    "        # Generate random \"text\" (just space-separated numbers as words)\n",
    "        num_words = np.random.randint(5, seq_length // 2)\n",
    "        words = [str(np.random.randint(1, vocab_size)) for _ in range(num_words)]\n",
    "        text = ' '.join(words)\n",
    "        sequences.append(text)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Define dataset parameters\n",
    "vocab_size = 5000\n",
    "max_seq_length = 50\n",
    "num_samples = 5000\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "\n",
    "# Generate mock dataset\n",
    "print(\"Generating mock dataset...\")\n",
    "all_texts = generate_mock_dataset(\n",
    "    num_samples=num_samples,\n",
    "    seq_length=max_seq_length,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Print a sample\n",
    "print(\"\\nSample text from dataset:\")\n",
    "print(all_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and DataLoaders\n",
    "\n",
    "Now let's create a dataset and dataloaders using our TextDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate function for batching\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Collate function for DataLoader.\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # For transformer training, we use the input shifted right as target\n",
    "    target = input_ids[:, 1:].contiguous()\n",
    "    source = input_ids[:, :-1].contiguous()\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextDataset(all_texts, tokenizer, max_length=max_seq_length)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Create the Model\n",
    "\n",
    "Let's configure and create our Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the default configuration\n",
    "config_manager = ConfigManager(default_config=get_default_config())\n",
    "config = config_manager.get_all()\n",
    "\n",
    "# Update configuration for our transformer model\n",
    "config_manager.set('transformer.vocab_size', vocab_size)\n",
    "config_manager.set('transformer.d_model', 128)  # Smaller for faster training\n",
    "config_manager.set('transformer.nhead', 4)\n",
    "config_manager.set('transformer.num_encoder_layers', 3)\n",
    "config_manager.set('transformer.num_decoder_layers', 3)\n",
    "config_manager.set('transformer.dim_feedforward', 512)\n",
    "config_manager.set('transformer.max_seq_length', max_seq_length)\n",
    "config_manager.set('model.dropout_rate', 0.1)\n",
    "config_manager.set('training.num_epochs', 3)  # Small number for demonstration\n",
    "config_manager.set('training.learning_rate', 0.0005)\n",
    "\n",
    "# Create model configuration\n",
    "model_config = {\n",
    "    'vocab_size': config['transformer']['vocab_size'],\n",
    "    'd_model': config['transformer']['d_model'],\n",
    "    'nhead': config['transformer']['nhead'],\n",
    "    'num_encoder_layers': config['transformer']['num_encoder_layers'],\n",
    "    'num_decoder_layers': config['transformer']['num_decoder_layers'],\n",
    "    'dim_feedforward': config['transformer']['dim_feedforward'],\n",
    "    'dropout': config['model']['dropout_rate'],\n",
    "    'max_seq_length': config['transformer']['max_seq_length']\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = TransformerModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Transformer Model created with {model.get_parameter_count():,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training\n",
    "\n",
    "We need to customize our forward pass for training the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original forward method\n",
    "original_forward = model.forward\n",
    "\n",
    "# Custom forward method for the trainer\n",
    "def train_forward(x):\n",
    "    # Unpack source and target from input\n",
    "    src, tgt = x\n",
    "    # Call the original forward method\n",
    "    output = original_forward(src, tgt)\n",
    "    # Reshape output for cross-entropy loss\n",
    "    batch_size, seq_len, vocab_size = output.size()\n",
    "    return output.reshape(batch_size * seq_len, vocab_size)\n",
    "\n",
    "# Monkey patch the forward method for training\n",
    "model.forward = train_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Trainer\n",
    "\n",
    "Now let's set up the training configuration and create our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer configuration\n",
    "trainer_config = {\n",
    "    'learning_rate': config['training']['learning_rate'],\n",
    "    'weight_decay': config['training']['weight_decay'],\n",
    "    'num_epochs': config['training']['num_epochs'],\n",
    "    'batch_size': batch_size,\n",
    "    'optimizer': 'adamw',  # Use AdamW optimizer\n",
    "    'scheduler': 'cosine',  # Use cosine annealing scheduler\n",
    "    'criterion': 'cross_entropy',  # Use cross-entropy loss\n",
    "    'clip_grad_norm': 1.0,  # Clip gradients\n",
    "    'early_stopping_patience': 3,  # Stop training if no improvement after 3 epochs\n",
    "    'checkpoint_dir': '../checkpoints/transformer',  # Directory to save model checkpoints\n",
    "    'save_best_only': True  # Only save the best model\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(trainer_config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(model, trainer_config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we're ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(f\"Starting training for {trainer_config['num_epochs']} epochs...\")\n",
    "stats = trainer.train(train_loader, val_loader)\n",
    "\n",
    "# Print best results\n",
    "print(f\"\\nBest validation accuracy: {stats['best_val_acc']:.2f}%\")\n",
    "print(f\"Best validation loss: {stats['best_val_loss']:.4f} (epoch {stats['best_epoch']})\")\n",
    "\n",
    "# Restore the original forward method\n",
    "model.forward = original_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Let's visualize how the training and validation metrics changed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(stats['train_loss']) + 1), stats['train_loss'], label='Training Loss')\n",
    "plt.plot(range(1, len(stats['val_loss']) + 1), stats['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(stats['train_acc']) + 1), stats['train_acc'], label='Training Accuracy')\n",
    "plt.plot(range(1, len(stats['val_acc']) + 1), stats['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = os.path.join(trainer_config['checkpoint_dir'], 'best_model.pt')\n",
    "model.load(best_model_path)\n",
    "\n",
    "# Create some seed sequences for text generation\n",
    "seed_texts = [\n",
    "    \"1000 2000 3000\",\n",
    "    \"500 600 700 800\",\n",
    "    \"100 200\"\n",
    "]\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, seed_text, tokenizer, max_length=30, temperature=1.0):\n",
    "    # Tokenize the seed text\n",
    "    encoding = tokenizer(seed_text, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    \n",
    "    # Generate sequence\n",
    "    generated_ids = model.generate(input_ids, max_length=max_length, temperature=temperature)\n",
    "    \n",
    "    # Convert token IDs back to words (in a real scenario, you would use the tokenizer's decode method)\n",
    "    # Here we just print the token IDs since our tokenizer is very simple\n",
    "    return generated_ids.cpu().numpy()\n",
    "\n",
    "# Generate text from each seed\n",
    "print(\"\\nGenerating text from seeds:\")\n",
    "model.eval()\n",
    "for i, seed_text in enumerate(seed_texts):\n",
    "    generated_ids = generate_text(model, seed_text, tokenizer)\n",
    "    print(f\"\\nSeed {i+1}: {seed_text}\")\n",
    "    print(f\"Generated sequence: {generated_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model Configuration\n",
    "\n",
    "Let's save the model configuration for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration to a file\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "config_path = '../outputs/transformer_config.yaml'\n",
    "config_manager.save_config(config_path)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how to use the neural networks framework to:\n",
    "\n",
    "1. Create a custom tokenizer and generate a mock text dataset\n",
    "2. Configure and create a Transformer model for text generation\n",
    "3. Train the model using our training utilities\n",
    "4. Generate text sequences using the trained model\n",
    "5. Save the model and configuration for future use\n",
    "\n",
    "For a real-world application, you would replace the mock dataset with actual text data and use a proper tokenizer like those from the Hugging Face `transformers` library. Additionally, you would likely train for more epochs and use a larger model for better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}