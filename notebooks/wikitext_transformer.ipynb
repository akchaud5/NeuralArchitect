{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Project: WikiText Language Modeling with Transformer\n",
    "\n",
    "This notebook demonstrates how to use the neural networks framework to train a Transformer model on the WikiText-2 dataset for language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.models.transformer_model import TransformerModel\n",
    "from src.utils.trainer import Trainer\n",
    "from src.config.config_manager import ConfigManager, get_default_config\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WikiText-2 Dataset\n",
    "\n",
    "WikiText-2 is a medium-sized dataset of Wikipedia articles, commonly used for language modeling tasks. It contains approximately 2 million tokens for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText-2 dataset from HuggingFace datasets\n",
    "wikitext_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(wikitext_dataset)\n",
    "\n",
    "# Look at the first few examples\n",
    "print(\"\\nSample text from training set:\")\n",
    "print(wikitext_dataset['train'][0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Tokenizer\n",
    "\n",
    "We'll use a pre-trained tokenizer from Hugging Face for processing the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer (using GPT-2 tokenizer as it's good for general text)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training\n",
    "\n",
    "Let's create a custom dataset class for handling WikiText data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, dataset_split, tokenizer, max_length=128):\n",
    "        self.dataset = dataset_split\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Filter out empty texts\n",
    "        self.texts = [text for text in self.dataset['text'] if len(text.strip()) > 0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, max_length=self.max_length, padding='max_length', \n",
    "                                 truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for key in encoding.keys():\n",
    "            encoding[key] = encoding[key].squeeze(0)\n",
    "            \n",
    "        return encoding\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = WikiTextDataset(wikitext_dataset['train'], tokenizer, max_length=max_length)\n",
    "val_dataset = WikiTextDataset(wikitext_dataset['validation'], tokenizer, max_length=max_length)\n",
    "test_dataset = WikiTextDataset(wikitext_dataset['test'], tokenizer, max_length=max_length)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "Now, let's create the dataloaders with a custom collate function for handling the transformer inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"Collate function for DataLoader.\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    # For transformer training, we use the input shifted right as target\n",
    "    target = input_ids[:, 1:].contiguous()\n",
    "    source = input_ids[:, :-1].contiguous()\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine a Training Batch\n",
    "\n",
    "Let's examine a batch of training data to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the training loader\n",
    "examples = iter(train_loader)\n",
    "source, target = next(examples)\n",
    "\n",
    "print(f\"Source shape: {source.shape}\")\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "\n",
    "# Show a single example\n",
    "example_idx = 0\n",
    "print(\"\\nExample input (source):\")\n",
    "print(tokenizer.decode(source[example_idx]))\n",
    "\n",
    "print(\"\\nExample target:\")\n",
    "print(tokenizer.decode(target[example_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Create the Model\n",
    "\n",
    "Let's configure and create our Transformer model for the WikiText language modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the default configuration\n",
    "config_manager = ConfigManager(default_config=get_default_config())\n",
    "config = config_manager.get_all()\n",
    "\n",
    "# Update configuration for the WikiText transformer model\n",
    "config_manager.set('transformer.vocab_size', vocab_size)\n",
    "config_manager.set('transformer.d_model', 256)  # Embedding dimension\n",
    "config_manager.set('transformer.nhead', 8)  # Number of attention heads\n",
    "config_manager.set('transformer.num_encoder_layers', 4)  # Number of encoder layers\n",
    "config_manager.set('transformer.num_decoder_layers', 4)  # Number of decoder layers\n",
    "config_manager.set('transformer.dim_feedforward', 1024)  # Dimension of feedforward network\n",
    "config_manager.set('transformer.max_seq_length', max_length)  # Maximum sequence length\n",
    "config_manager.set('model.dropout_rate', 0.1)  # Dropout rate\n",
    "config_manager.set('training.num_epochs', 5)  # Number of epochs (reduced for demonstration)\n",
    "config_manager.set('training.learning_rate', 2e-4)  # Learning rate\n",
    "config_manager.set('training.weight_decay', 1e-4)  # Weight decay for regularization\n",
    "\n",
    "# Create model configuration\n",
    "model_config = {\n",
    "    'vocab_size': config['transformer']['vocab_size'],\n",
    "    'd_model': config['transformer']['d_model'],\n",
    "    'nhead': config['transformer']['nhead'],\n",
    "    'num_encoder_layers': config['transformer']['num_encoder_layers'],\n",
    "    'num_decoder_layers': config['transformer']['num_decoder_layers'],\n",
    "    'dim_feedforward': config['transformer']['dim_feedforward'],\n",
    "    'dropout': config['model']['dropout_rate'],\n",
    "    'max_seq_length': config['transformer']['max_seq_length'],\n",
    "    'pad_idx': tokenizer.pad_token_id\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = TransformerModel(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Transformer Model created with {model.get_parameter_count():,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training\n",
    "\n",
    "We need to customize our forward pass for training the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original forward method\n",
    "original_forward = model.forward\n",
    "\n",
    "# Custom forward method for the trainer\n",
    "def train_forward(x):\n",
    "    # Unpack source and target from input\n",
    "    src, tgt = x\n",
    "    # Call the original forward method\n",
    "    output = original_forward(src, tgt)\n",
    "    # Reshape output for cross-entropy loss\n",
    "    batch_size, seq_len, vocab_size = output.size()\n",
    "    return output.reshape(batch_size * seq_len, vocab_size)\n",
    "\n",
    "# Monkey patch the forward method for training\n",
    "model.forward = train_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Trainer\n",
    "\n",
    "Now let's set up the training configuration and create our trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer configuration\n",
    "trainer_config = {\n",
    "    'learning_rate': config['training']['learning_rate'],\n",
    "    'weight_decay': config['training']['weight_decay'],\n",
    "    'num_epochs': config['training']['num_epochs'],\n",
    "    'batch_size': batch_size,\n",
    "    'optimizer': 'adamw',  # Use AdamW optimizer\n",
    "    'scheduler': 'cosine',  # Use cosine annealing scheduler\n",
    "    'criterion': 'cross_entropy',  # Use cross-entropy loss\n",
    "    'clip_grad_norm': 1.0,  # Clip gradients\n",
    "    'early_stopping_patience': 2,  # Stop training if no improvement after 2 epochs\n",
    "    'checkpoint_dir': '../checkpoints/wikitext',  # Directory to save model checkpoints\n",
    "    'save_best_only': True  # Only save the best model\n",
    "}\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(trainer_config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(model, trainer_config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we're ready to train our model on WikiText-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(f\"Starting training for {trainer_config['num_epochs']} epochs...\")\n",
    "stats = trainer.train(train_loader, val_loader)\n",
    "\n",
    "# Print best results\n",
    "print(f\"\\nBest validation accuracy: {stats['best_val_acc']:.2f}%\")\n",
    "print(f\"Best validation loss: {stats['best_val_loss']:.4f} (epoch {stats['best_epoch']})\")\n",
    "\n",
    "# Restore the original forward method\n",
    "model.forward = original_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Let's visualize how the training and validation metrics changed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss/accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(stats['train_loss']) + 1), stats['train_loss'], label='Training Loss')\n",
    "plt.plot(range(1, len(stats['val_loss']) + 1), stats['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(stats['train_acc']) + 1), stats['train_acc'], label='Training Accuracy')\n",
    "plt.plot(range(1, len(stats['val_acc']) + 1), stats['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity\n",
    "\n",
    "Perplexity is a common metric for evaluating language models. It is the exponentiated average negative log-likelihood of a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in data_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output_flat = output.view(-1, output.size(-1))\n",
    "            tgt_flat = tgt.reshape(-1)\n",
    "            loss = torch.nn.functional.cross_entropy(output_flat, tgt_flat, reduction='sum', ignore_index=tokenizer.pad_token_id)\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            non_pad_mask = tgt_flat != tokenizer.pad_token_id\n",
    "            num_tokens = non_pad_mask.sum().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Load the best model\n",
    "best_model_path = os.path.join(trainer_config['checkpoint_dir'], 'best_model.pt')\n",
    "model.load(best_model_path)\n",
    "\n",
    "# Calculate perplexity on validation and test sets\n",
    "val_perplexity = calculate_perplexity(model, val_loader, device)\n",
    "test_perplexity = calculate_perplexity(model, test_loader, device)\n",
    "\n",
    "print(f\"Validation Perplexity: {val_perplexity:.2f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with the Trained Model\n",
    "\n",
    "Let's use our trained model to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, tokenizer, max_length=50, temperature=1.0):\n",
    "    # Tokenize the prompt\n",
    "    encoding = tokenizer(prompt, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    \n",
    "    # Generate sequence\n",
    "    generated_ids = model.generate(input_ids, max_length=max_length, temperature=temperature)\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Define some prompts for text generation\n",
    "prompts = [\n",
    "    \"The history of artificial intelligence\",\n",
    "    \"Neural networks are\",\n",
    "    \"The capital city of France is Paris, which\"\n",
    "]\n",
    "\n",
    "# Generate text with different temperature settings\n",
    "temperatures = [0.7, 1.0, 1.3]\n",
    "\n",
    "model.eval()\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    for temp in temperatures:\n",
    "        generated = generate_text(model, prompt, tokenizer, max_length=50, temperature=temp)\n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Specific Examples\n",
    "\n",
    "Let's evaluate our model on specific examples to analyze its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction(model, text, tokenizer, device):\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    \n",
    "    # Shift input and target for language modeling\n",
    "    src = input_ids[:, :-1]\n",
    "    tgt = input_ids[:, 1:]\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(src, tgt)\n",
    "    \n",
    "    # Get the predicted token at each position\n",
    "    predicted_ids = torch.argmax(output, dim=-1)\n",
    "    \n",
    "    # Decode the tokens\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(src[0].tolist())\n",
    "    target_tokens = tokenizer.convert_ids_to_tokens(tgt[0].tolist())\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_ids[0].tolist())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (predicted_ids == tgt).sum().item()\n",
    "    total = tgt.numel()\n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    return {\n",
    "        'input_tokens': input_tokens,\n",
    "        'target_tokens': target_tokens,\n",
    "        'predicted_tokens': predicted_tokens,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Define some example texts to analyze\n",
    "analysis_texts = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Neural networks are a subset of machine learning.\",\n",
    "    \"In computer science, artificial intelligence refers to intelligence demonstrated by machines.\"\n",
    "]\n",
    "\n",
    "# Analyze predictions\n",
    "for text in analysis_texts:\n",
    "    print(f\"\\nAnalyzing: '{text}'\")\n",
    "    result = analyze_prediction(model, text, tokenizer, device)\n",
    "    \n",
    "    print(f\"Prediction accuracy: {result['accuracy']:.2f}%\")\n",
    "    \n",
    "    # Print a table of tokens\n",
    "    print(\"\\nToken  |  Target  |  Predicted\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(len(result['input_tokens'])):\n",
    "        input_token = result['input_tokens'][i]\n",
    "        target_token = result['target_tokens'][i] if i < len(result['target_tokens']) else 'N/A'\n",
    "        pred_token = result['predicted_tokens'][i] if i < len(result['predicted_tokens']) else 'N/A'\n",
    "        \n",
    "        # Format to handle special tokens better\n",
    "        input_token = input_token.replace('Ġ', '').replace('Ċ', '\\n')\n",
    "        target_token = target_token.replace('Ġ', '').replace('Ċ', '\\n')\n",
    "        pred_token = pred_token.replace('Ġ', '').replace('Ċ', '\\n')\n",
    "        \n",
    "        # Mark correct/incorrect predictions\n",
    "        mark = \"✓\" if target_token == pred_token else \"✗\"\n",
    "        \n",
    "        print(f\"{input_token:10} | {target_token:10} | {pred_token:10} {mark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model Configuration\n",
    "\n",
    "Let's save the model configuration for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the configuration to a file\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "config_path = '../outputs/wikitext_transformer_config.yaml'\n",
    "config_manager.save_config(config_path)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated how to use the neural networks framework to:\n",
    "\n",
    "1. Load and preprocess the WikiText-2 dataset from HuggingFace\n",
    "2. Use a pre-trained tokenizer for text processing\n",
    "3. Configure and create a Transformer model for language modeling\n",
    "4. Train the model and evaluate with perplexity\n",
    "5. Generate text using the trained model\n",
    "6. Analyze the model's predictions on specific examples\n",
    "\n",
    "Language modeling is a challenging task that typically requires larger models and more training data for state-of-the-art results. This implementation demonstrates the core concepts and can be extended with more sophisticated architectural elements like adapter layers, larger models, or pre-training and fine-tuning approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}